Reflection Report

AI Development Skills Applied

Prompt Engineering
- I learned to break large feature requests into smaller, promptable workflows.
- Prompting Cursor with context (existing features, tech stack, design choices) produced more accurate and usable output.
- The most effective prompts were ones that asked for both **code and explanation** — helping me learn while building.

Tool Orchestration
- My primary AI tool was **Cursor IDE**, which integrated AI seamlessly into my dev workflow.
- I used it alongside manual testing tools like Postman and browser devtools.
- Cursor acted like an assistant, while I focused on stitching everything together.

Quality Validation
- Since automated tests were limited, I validated AI-generated code by:
  - Reviewing line-by-line logic
  - Running Postman requests and debugging with backend logs
  - Manually testing the full frontend flows to catch edge cases

Business Value Delivered

Functional Requirements
- ~90% of the core requirements were delivered.
- Trade-offs included skipping real-time infra and test automation to prioritize deployable, visible functionality.

User Experience
- AI helped me build reusable, clean React components.
- Real-time-feel features like polling notifications added interactivity without overcomplicating the stack.

Code Quality
- Cursor followed best practices (DTOs, REST patterns, service separation).
- I used Flyway to version DB changes, JWT for secure auth, and role-based access for maintainability and scalability.

Learnings

Most Valuable AI Technique
- Breaking features into clear, contextual prompts** with examples of input/output structures.
- Cursor’s ability to convert verbal workflows into backend entities, services, and React forms was extremely useful.

Biggest Challenge
- AI struggled with building UI, test scaffolding, JWT validation testing, and context-carryover between prompts (especially if not re-supplied).

Process Improvements
- I would spend more time structuring prompt libraries early in the project.
- Starting with test setup on day one would have made validation smoother.

Knowledge Gained
- I learned practical techniques for building full-stack systems quickly using AI.
- I also got deeper understanding of how to structure real-world features like:
  - Role-based access control
  - Poll-based notification workflows
  - Time tracking in teams

Future Application

Team Integration
- I would teach teammates to use AI tools like Cursor to speed up boilerplate and architecture scaffolding.

Process Enhancement
- Future teams could maintain a shared prompt library for common tasks.
- Integrating AI review into PRs (Pull Requests) could assist with refactoring and documentation.

Scaling Considerations
- With the right conventions, AI-assisted development can scale across enterprise-level modules.
- Teams could split features, auto-generate scaffolding, and still maintain consistency using shared AI guidance and base prompts.

